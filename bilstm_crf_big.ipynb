{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cd815b",
   "metadata": {},
   "source": [
    "# Bi-LSTM Conditional Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8435509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18b22ac8610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2fc78",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d90b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] if w in to_ix.keys() else len(to_ix) for w in seq ]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324b61",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dcbac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49de6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size+1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c0f79",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74d82c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCS = ['fr_FR', 'fr_FR', 'fr_FR', 'en_US', 'en_GB', 'de_DE', 'fr_CH', 'nl_BE', 'it_IT', 'es_ES']\n",
    "fake = {loc:Faker(loc) for loc in LOCS}\n",
    "Faker.seed(411)\n",
    "\n",
    "DATASET_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb25f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adrs = []\n",
    "\n",
    "for i in range(DATASET_SIZE):\n",
    "    words, tags = [],[]\n",
    "    loc = random.sample(LOCS,1)[0]\n",
    "    \n",
    "    for f in fake[loc].iban().split():\n",
    "        if random.random()>0.2:\n",
    "            words.append(f)\n",
    "            tags.append('IBAN')\n",
    "        \n",
    "    if random.random() > 0.5:\n",
    "        for f in fake[loc].name().split():\n",
    "            words.append(f)\n",
    "            tags.append('NAME')\n",
    "    else:\n",
    "        for f in fake[loc].company().split():\n",
    "            words.append(f)\n",
    "            tags.append('ORG')\n",
    "        \n",
    "    for f in fake[loc].address().split():\n",
    "        if random.random()>0.1:\n",
    "            words.append(f)\n",
    "            tags.append('ADDRESS')\n",
    "    \n",
    "    if random.random()>0.1:\n",
    "        words.append(loc[-2:])\n",
    "        tags.append('COUNTRY')\n",
    "        \n",
    "    adrs.append((words,tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81146308",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8f38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(adrs) * 0.8)\n",
    "training_data = adrs[:split]\n",
    "valid_data = adrs[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4795dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for adr, tags in training_data:\n",
    "    for word in adr:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"IBAN\": 0, \"NAME\": 1, \"ORG\": 2, \"ADDRESS\": 3, \"COUNTRY\": 4, START_TAG: 5, STOP_TAG: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f732a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e66751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_CRF(\n",
      "  (word_embeds): Embedding(29283, 256)\n",
      "  (lstm): LSTM(256, 256, bidirectional=True)\n",
      "  (hidden2tag): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf179ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(14.3086), [3, 0, 1, 0, 1, 0, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21a4e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(training_data):\n",
    "        sentence, tags = data\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        disp_step = len(training_data) / 5\n",
    "        if i % disp_step == disp_step - 1:\n",
    "            last_loss = running_loss / disp_step # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_data) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8df62a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1600 loss: 3.2347769457101823\n",
      "  batch 3200 loss: 1.685337679386139\n",
      "  batch 4800 loss: 1.4260619646310806\n",
      "  batch 6400 loss: 1.2517214620113373\n",
      "  batch 8000 loss: 1.2579562866687775\n",
      "LOSS train 1.2579562866687775 valid tensor([1.0847], grad_fn=<DivBackward0>)\n",
      "EPOCH 2:\n",
      "  batch 1600 loss: 0.9252439570426941\n",
      "  batch 3200 loss: 0.8165388548374176\n",
      "  batch 4800 loss: 0.7426926696300507\n",
      "  batch 6400 loss: 0.6978679418563842\n",
      "  batch 8000 loss: 0.7020111620426178\n",
      "LOSS train 0.7020111620426178 valid tensor([0.7946], grad_fn=<DivBackward0>)\n",
      "EPOCH 3:\n",
      "  batch 1600 loss: 0.5893281841278076\n",
      "  batch 3200 loss: 0.48007638454437257\n",
      "  batch 4800 loss: 0.39132469534873965\n",
      "  batch 6400 loss: 0.3825351691246033\n",
      "  batch 8000 loss: 0.37446933627128604\n",
      "LOSS train 0.37446933627128604 valid tensor([0.7250], grad_fn=<DivBackward0>)\n",
      "EPOCH 4:\n",
      "  batch 1600 loss: 0.3115292310714722\n",
      "  batch 3200 loss: 0.24027343988418579\n",
      "  batch 4800 loss: 0.19763081073760985\n",
      "  batch 6400 loss: 0.18912353515625\n",
      "  batch 8000 loss: 0.17654456734657287\n",
      "LOSS train 0.17654456734657287 valid tensor([0.7227], grad_fn=<DivBackward0>)\n",
      "EPOCH 5:\n",
      "  batch 1600 loss: 0.14822123765945436\n",
      "  batch 3200 loss: 0.10981809377670287\n",
      "  batch 4800 loss: 0.0916771936416626\n",
      "  batch 6400 loss: 0.08501060962677003\n",
      "  batch 8000 loss: 0.07110276579856872\n",
      "LOSS train 0.07110276579856872 valid tensor([0.7670], grad_fn=<DivBackward0>)\n",
      "EPOCH 6:\n",
      "  batch 1600 loss: 0.06791123151779174\n",
      "  batch 3200 loss: 0.05500017642974853\n",
      "  batch 4800 loss: 0.04856745600700378\n",
      "  batch 6400 loss: 0.043690849542617795\n",
      "  batch 8000 loss: 0.038386077880859376\n",
      "LOSS train 0.038386077880859376 valid tensor([0.7357], grad_fn=<DivBackward0>)\n",
      "EPOCH 7:\n",
      "  batch 1600 loss: 0.03158464193344116\n",
      "  batch 3200 loss: 0.027976419925689697\n",
      "  batch 4800 loss: 0.02736788511276245\n",
      "  batch 6400 loss: 0.028508546352386473\n",
      "  batch 8000 loss: 0.02354123592376709\n",
      "LOSS train 0.02354123592376709 valid tensor([0.7149], grad_fn=<DivBackward0>)\n",
      "EPOCH 8:\n",
      "  batch 1600 loss: 0.0214329195022583\n",
      "  batch 3200 loss: 0.017009198665618896\n",
      "  batch 4800 loss: 0.01906298637390137\n",
      "  batch 6400 loss: 0.01646123766899109\n",
      "  batch 8000 loss: 0.014321551322937012\n",
      "LOSS train 0.014321551322937012 valid tensor([0.7376], grad_fn=<DivBackward0>)\n",
      "EPOCH 9:\n",
      "  batch 1600 loss: 0.012825934886932374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     15\u001b[0m \u001b[39m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mneg_log_likelihood(sentence_in, targets)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# calling optimizer.step()\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Shadow\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    \n",
    "    for i, vdata in enumerate(valid_data):\n",
    "        sentence, tags = vdata\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        vloss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "838d4e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b10227b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Armando Campoy Pellicer Pasadizo Celestina Roca 3 Puerta 1 30625 ES\n",
      "target:  NAME NAME NAME ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN NAME NAME ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "BE70680129240538 Kobe Bekaert Carineboulevard 4 Grand-Leez BE\n",
      "target:  IBAN NAME NAME ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN NAME NAME ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "FR6101408028978985394363446 Boulay 451, avenue de Jacquot 33556 RocherBourg FR\n",
      "target:  IBAN ORG ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN ORG ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "CH2710617282823352601 Cattin Raymond 1577 Chenaux-sur-Sansonnens CH\n",
      "target:  IBAN ORG ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN NAME NAME ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "FR6683917370006281123462920 Parent avenue Mend√®s 15260 Royernec FR\n",
      "target:  IBAN ORG ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN ORG ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    for i in random.sample(range(len(valid_data)),5):\n",
    "        precheck_sent = prepare_sequence(valid_data[i][0], word_to_ix)\n",
    "        pred = model(precheck_sent)\n",
    "\n",
    "        print(\" \".join(valid_data[i][0]))\n",
    "        print(\"target: \" ,\" \".join(valid_data[i][1]))\n",
    "        print(\"predic: \" ,\" \".join([ix_to_tag[p] for p in pred[1]]))\n",
    "        print(\"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "756f06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in valid_data[:100]:\n",
    "        precheck_sent = prepare_sequence(data[0], word_to_ix)\n",
    "        preds += [ix_to_tag[p] for p in model(precheck_sent)[1]]\n",
    "        targets += data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47bbe096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Local\\Temp\\ipykernel_4212\\4276173271.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df.groupby('preds').mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADDRESS</th>\n",
       "      <td>0.989150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTRY</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBAN</th>\n",
       "      <td>0.885057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME</th>\n",
       "      <td>0.927928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.926230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          success\n",
       "preds            \n",
       "ADDRESS  0.989150\n",
       "COUNTRY  1.000000\n",
       "IBAN     0.885057\n",
       "NAME     0.927928\n",
       "ORG      0.926230"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'preds':preds, 'targets':targets})\n",
    "df[\"success\"] = pd.to_numeric(df.preds == df.targets)\n",
    "df.groupby('preds').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a97f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
