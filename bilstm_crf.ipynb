{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cd815b",
   "metadata": {},
   "source": [
    "# Bi-LSTM Conditional Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8435509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22715d545f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2fc78",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d90b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] if w in to_ix.keys() else len(to_ix) for w in seq ]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324b61",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49de6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size+1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c0f79",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d82c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCS = ['fr_FR', 'fr_FR', 'fr_FR', 'de_DE', 'it_IT','en_US', 'fr_CH', 'nl_BE', 'ro_RO', 'ru_RU', 'zh_CN']\n",
    "fake = {loc:Faker(loc) for loc in LOCS}\n",
    "Faker.seed(411)\n",
    "\n",
    "DATASET_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb25f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adrs = []\n",
    "\n",
    "for i in range(DATASET_SIZE):\n",
    "    words, tags = [],[]\n",
    "    loc = random.sample(LOCS,1)[0]\n",
    "    \n",
    "    for f in fake[loc].iban().split():\n",
    "        if random.random()>0.2:\n",
    "            words.append(f)\n",
    "            tags.append('IBAN')\n",
    "        \n",
    "    if random.random() > 0.5:\n",
    "        for f in fake[loc].name().split():\n",
    "            words.append(f)\n",
    "            tags.append('NAME')\n",
    "    else:\n",
    "        for f in fake[loc].company().split():\n",
    "            words.append(f)\n",
    "            tags.append('ORG')\n",
    "        \n",
    "    for f in fake[loc].address().split():\n",
    "        if random.random()>0.1:\n",
    "            words.append(f)\n",
    "            tags.append('ADDRESS')\n",
    "    \n",
    "    if random.random()>0.1:\n",
    "        words.append(loc[-2:])\n",
    "        tags.append('COUNTRY')\n",
    "        \n",
    "    adrs.append((words,tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81146308",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8558fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8f38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(adrs) * 0.8)\n",
    "training_data = adrs[:split]\n",
    "valid_data = adrs[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4795dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for adr, tags in training_data:\n",
    "    for word in adr:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"IBAN\": 0, \"NAME\": 1, \"ORG\": 2, \"ADDRESS\": 3, \"COUNTRY\": 4, START_TAG: 5, STOP_TAG: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f732a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e66751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_CRF(\n",
      "  (word_embeds): Embedding(16588, 64)\n",
      "  (lstm): LSTM(64, 128, bidirectional=True)\n",
      "  (hidden2tag): Linear(in_features=256, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf179ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(6.4928), [1, 4, 1, 4, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a4e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(training_data):\n",
    "        sentence, tags = data\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        disp_step = len(training_data) / 5\n",
    "        if i % disp_step == disp_step - 1:\n",
    "            last_loss = running_loss / disp_step # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_data) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "            \n",
    "    return last_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df62a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 800 loss: 5.184063190817833\n",
      "  batch 1600 loss: 2.673265378475189\n",
      "  batch 2400 loss: 2.3263999181985855\n",
      "  batch 3200 loss: 2.1676559525728227\n",
      "  batch 4000 loss: 2.0790936303138734\n",
      "LOSS train 2.0790936303138734 valid tensor([2.1796], grad_fn=<DivBackward0>)\n",
      "EPOCH 2:\n",
      "  batch 800 loss: 1.9767471635341645\n",
      "  batch 1600 loss: 1.814931811094284\n",
      "  batch 2400 loss: 1.6406215167045592\n",
      "  batch 3200 loss: 1.5203109323978423\n",
      "  batch 4000 loss: 1.517509219646454\n",
      "LOSS train 1.517509219646454 valid tensor([1.8025], grad_fn=<DivBackward0>)\n",
      "EPOCH 3:\n",
      "  batch 800 loss: 1.5359493625164031\n",
      "  batch 1600 loss: 1.362930200099945\n",
      "  batch 2400 loss: 1.2150639700889587\n",
      "  batch 3200 loss: 1.2055588138103486\n",
      "  batch 4000 loss: 1.2074001348018646\n",
      "LOSS train 1.2074001348018646 valid tensor([1.3450], grad_fn=<DivBackward0>)\n",
      "EPOCH 4:\n",
      "  batch 800 loss: 1.2223499071598054\n",
      "  batch 1600 loss: 1.0354771995544434\n",
      "  batch 2400 loss: 1.020994462966919\n",
      "  batch 3200 loss: 0.9704701733589173\n",
      "  batch 4000 loss: 0.9562414515018464\n",
      "LOSS train 0.9562414515018464 valid tensor([1.4315], grad_fn=<DivBackward0>)\n",
      "EPOCH 5:\n",
      "  batch 800 loss: 1.0112985950708389\n",
      "  batch 1600 loss: 0.8737814384698868\n",
      "  batch 2400 loss: 0.829767233133316\n",
      "  batch 3200 loss: 0.7638986647129059\n",
      "  batch 4000 loss: 0.7440233421325684\n",
      "LOSS train 0.7440233421325684 valid tensor([1.3232], grad_fn=<DivBackward0>)\n",
      "EPOCH 6:\n",
      "  batch 800 loss: 0.7711053591966629\n",
      "  batch 1600 loss: 0.7025028985738754\n",
      "  batch 2400 loss: 0.6691644525527954\n",
      "  batch 3200 loss: 0.6303715872764587\n",
      "  batch 4000 loss: 0.624642049074173\n",
      "LOSS train 0.624642049074173 valid tensor([1.3890], grad_fn=<DivBackward0>)\n",
      "EPOCH 7:\n",
      "  batch 800 loss: 0.6438516414165497\n",
      "  batch 1600 loss: 0.5498427355289459\n",
      "  batch 2400 loss: 0.4962625873088837\n",
      "  batch 3200 loss: 0.4642479467391968\n",
      "  batch 4000 loss: 0.46220916986465455\n",
      "LOSS train 0.46220916986465455 valid tensor([1.9224], grad_fn=<DivBackward0>)\n",
      "EPOCH 8:\n",
      "  batch 800 loss: 0.48362151622772215\n",
      "  batch 1600 loss: 0.3956798565387726\n",
      "  batch 2400 loss: 0.34921082854270935\n",
      "  batch 3200 loss: 0.3807057332992554\n",
      "  batch 4000 loss: 0.34390795946121216\n",
      "LOSS train 0.34390795946121216 valid tensor([1.7127], grad_fn=<DivBackward0>)\n",
      "EPOCH 9:\n",
      "  batch 800 loss: 0.3697774946689606\n",
      "  batch 1600 loss: 0.2852416408061981\n",
      "  batch 2400 loss: 0.286531548500061\n",
      "  batch 3200 loss: 0.2546337985992432\n",
      "  batch 4000 loss: 0.24007462859153747\n",
      "LOSS train 0.24007462859153747 valid tensor([1.9715], grad_fn=<DivBackward0>)\n",
      "EPOCH 10:\n",
      "  batch 800 loss: 0.296784051656723\n",
      "  batch 1600 loss: 0.20828771829605103\n",
      "  batch 2400 loss: 0.19066046118736268\n",
      "  batch 3200 loss: 0.17414599657058716\n",
      "  batch 4000 loss: 0.16290411949157715\n",
      "LOSS train 0.16290411949157715 valid tensor([2.4256], grad_fn=<DivBackward0>)\n",
      "EPOCH 11:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m     14\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tag_to_ix[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tags], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 3. Run our forward pass.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_log_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# calling optimizer.step()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[3], line 129\u001b[0m, in \u001b[0;36mBiLSTM_CRF.neg_log_likelihood\u001b[1;34m(self, sentence, tags)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneg_log_likelihood\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, tags):\n\u001b[0;32m    128\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lstm_features(sentence)\n\u001b[1;32m--> 129\u001b[0m     forward_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_alg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     gold_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_sentence(feats, tags)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_score \u001b[38;5;241m-\u001b[39m gold_score\n",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m, in \u001b[0;36mBiLSTM_CRF._forward_alg\u001b[1;34m(self, feats)\u001b[0m\n\u001b[0;32m     56\u001b[0m         next_tag_var \u001b[38;5;241m=\u001b[39m forward_var \u001b[38;5;241m+\u001b[39m trans_score \u001b[38;5;241m+\u001b[39m emit_score\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;66;03m# The forward variable for this tag is log-sum-exp of all the\u001b[39;00m\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;66;03m# scores.\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m         alphas_t\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlog_sum_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_tag_var\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     60\u001b[0m     forward_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(alphas_t)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m terminal_var \u001b[38;5;241m=\u001b[39m forward_var \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_to_ix[STOP_TAG]]\n",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m, in \u001b[0;36mlog_sum_exp\u001b[1;34m(vec)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_sum_exp\u001b[39m(vec):\n\u001b[0;32m     14\u001b[0m     max_score \u001b[38;5;241m=\u001b[39m vec[\u001b[38;5;241m0\u001b[39m, argmax(vec)]\n\u001b[1;32m---> 15\u001b[0m     max_score_broadcast \u001b[38;5;241m=\u001b[39m \u001b[43mmax_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m1\u001b[39m, vec\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_score \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     17\u001b[0m         torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mexp(vec \u001b[38;5;241m-\u001b[39m max_score_broadcast)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    \n",
    "    \n",
    "    for i, vdata in enumerate(valid_data):\n",
    "        sentence, tags = vdata\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        vloss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7259ef20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model.load_state_dict(torch.load(\"model_20230401_104345_4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b10227b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE87303905806726 Verlinden CommV Ivanweg 082 5259 Goutroux\n",
      "target:  IBAN ORG ORG ADDRESS ADDRESS ADDRESS ADDRESS\n",
      "predic:  IBAN ORG ORG ADDRESS ADDRESS ADDRESS ADDRESS\n",
      "******\n",
      "RU19AOPW5554876779062 Игнатьева Виктория Матвеевна ст. Русса, пр. д. 197 65, 324872 RU\n",
      "target:  IBAN NAME NAME NAME ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN NAME NAME NAME ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "RO78LBKY0572183817677456 Ababei Dumitrescu INC Soseaua Oprea Nr. 593 Sancraiu de Mures, 765599 RO\n",
      "target:  IBAN ORG ORG ORG ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN ORG ORG ORG ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "RO27INNV1083166516774525 Adriana Popescu Strada Stancu Buhusi, 525239 RO\n",
      "target:  IBAN NAME NAME ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN ORG ORG ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n",
      "FR9287520701483200950568562 Faure 750, rue de 82124 Buissonnec FR\n",
      "target:  IBAN ORG ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "predic:  IBAN NAME ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS COUNTRY\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "\n",
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    for i in random.sample(range(len(valid_data)),5):\n",
    "        precheck_sent = prepare_sequence(valid_data[i][0], word_to_ix)\n",
    "        pred = model(precheck_sent)\n",
    "\n",
    "        print(\" \".join(valid_data[i][0]))\n",
    "        print(\"target: \" ,\" \".join(valid_data[i][1]))\n",
    "        print(\"predic: \" ,\" \".join([ix_to_tag[p] for p in pred[1]]))\n",
    "        print(\"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "756f06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for data in valid_data[:100]:\n",
    "        precheck_sent = prepare_sequence(data[0], word_to_ix)\n",
    "        preds += [ix_to_tag[p] for p in model(precheck_sent)[1]]\n",
    "        targets += data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47bbe096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\AppData\\Local\\Temp\\ipykernel_27040\\4276173271.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df.groupby('preds').mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADDRESS</th>\n",
       "      <td>0.966608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTRY</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBAN</th>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME</th>\n",
       "      <td>0.858824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.812950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          success\n",
       "preds            \n",
       "ADDRESS  0.966608\n",
       "COUNTRY  1.000000\n",
       "IBAN     0.871795\n",
       "NAME     0.858824\n",
       "ORG      0.812950"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'preds':preds, 'targets':targets})\n",
    "df[\"success\"] = pd.to_numeric(df.preds == df.targets)\n",
    "df.groupby('preds').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13701395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794d13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
